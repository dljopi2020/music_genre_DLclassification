{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"CNN_and_LSTM_code.ipynb","provenance":[{"file_id":"1ckJUPsT7uerhk65CKKL_vQ1iPO4E94N5","timestamp":1588241087133}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"z-ZXiTsReopJ","colab_type":"code","outputId":"e16a86d4-e31a-4857-c537-aa8895e28c44","executionInfo":{"status":"ok","timestamp":1588756910717,"user_tz":-120,"elapsed":7845,"user":{"displayName":"DL jopi","photoUrl":"","userId":"13040650484293272934"}},"colab":{"base_uri":"https://localhost:8080/","height":176}},"source":["import librosa\n","import os\n","import glob\n","import IPython.display as ipd\n","from pathlib import Path\n","import timeit\n","import time, sys\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import librosa.display\n","\n","import pandas as pd\n","from sklearn import datasets, linear_model\n","from sklearn.model_selection import train_test_split\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import cv2\n","import seaborn as sns\n","\n","%tensorflow_version 1.x #version 1 works without problems\n","import tensorflow\n","\n","\n","from sklearn.preprocessing import LabelEncoder\n","from keras.utils import to_categorical\n","\n","import keras\n","from keras.models import Sequential\n","from keras.callbacks import EarlyStopping\n","from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n","from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, BatchNormalization, Activation, GaussianNoise, LSTM\n","from sklearn.metrics import accuracy_score\n","\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"stream","text":["`%tensorflow_version` only switches the major version: 1.x or 2.x.\n","You set: `1.x #version 1 works without problems`. This will be interpreted as: `1.x`.\n","\n","\n","TensorFlow 1.x selected.\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"uDrq2QCx0OG0","colab_type":"text"},"source":["# Access data in drive and make ready for use"]},{"cell_type":"code","metadata":{"id":"NqSY7iAGE9PZ","colab_type":"code","outputId":"b315a9ca-eed0-4cdd-afce-9772caf45241","executionInfo":{"status":"ok","timestamp":1588756938152,"user_tz":-120,"elapsed":24399,"user":{"displayName":"DL jopi","photoUrl":"","userId":"13040650484293272934"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["#authorize colab to use data in google drive\n","from google.colab import drive \n","drive.mount('/content/gdrive') "],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zAqQEwmgeopQ","colab_type":"code","outputId":"9336f1d4-ea6c-4093-9c95-20675a5dc54f","executionInfo":{"status":"ok","timestamp":1588756940353,"user_tz":-120,"elapsed":781,"user":{"displayName":"DL jopi","photoUrl":"","userId":"13040650484293272934"}},"colab":{"base_uri":"https://localhost:8080/","height":191}},"source":["#create path to dataset and check if it finds genres\n","DATA_DIR = Path('/content/gdrive/My Drive/genres/') \n","genres = [x.name for x in DATA_DIR.glob('*') if x.is_dir()]\n","genres"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['blues',\n"," 'rock',\n"," 'classical',\n"," 'disco',\n"," 'hiphop',\n"," 'metal',\n"," 'pop',\n"," 'jazz',\n"," 'country',\n"," 'reggae']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"uYJOL-_Neopg","colab_type":"code","colab":{}},"source":["def create_paths_ds(paths: Path, label: str) -> list:\n","    EXTENSION_TYPE = '.wav'\n","    return [(x, label) for x in paths.glob('*' + EXTENSION_TYPE)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fcsMxKAbeopY","colab_type":"code","outputId":"9fe9d51a-a31b-4c28-cfa8-d4b106044f86","executionInfo":{"status":"ok","timestamp":1588756945837,"user_tz":-120,"elapsed":866,"user":{"displayName":"DL jopi","photoUrl":"","userId":"13040650484293272934"}},"colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["categories_to_use = [ #choose the genres used \n","    'classical',\n","    'hiphop',\n","    'metal',\n","    'country',\n","    # 'reggae'\n","]\n","\n","#overwriting classes to all classes\n","# categories_to_use = categories\n","\n","NUM_CLASSES = len(categories_to_use)\n","\n","print(f'Number of classes: {NUM_CLASSES}')\n","\n","paths_all_labels = []\n","for cat in categories_to_use:\n","    paths_all_labels += create_paths_ds(DATA_DIR / cat, cat)\n","\n","\n","X_train, X_test = train_test_split(paths_all_labels,test_size=0.1)\n","X_train, X_val = train_test_split(X_train, test_size=0.2)\n","\n","print(f'Train length: {len(X_train)}')\n","print(f'Validation length: {len(X_val)}')\n","print(f'Test length: {len(X_test)}')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Number of classes: 4\n","Train length: 288\n","Validation length: 72\n","Test length: 40\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vKBN9_romai4","colab_type":"code","colab":{}},"source":["def load_and_preprocess_cnn(dataset, SAMPLE_SIZE = 30):\n","    IMG_SIZE = (216,128) \n","    progress = 0\n","\n","    data = []\n","    labels = []\n","    for (path, label) in dataset:\n","        audio, sr = librosa.load(path)\n","        dur = librosa.get_duration(audio, sr = sr)\n","        sampleNum = int(dur / SAMPLE_SIZE)\n","        offset = (dur % SAMPLE_SIZE) / 2\n","        for i in range(sampleNum):\n","            audio, sr = librosa.load(path, offset= offset+i, duration=SAMPLE_SIZE)\n","            sample = librosa.feature.melspectrogram(audio, sr=sr)\n","            # print(sample.shape)\n","            sample = cv2.resize(sample, dsize=IMG_SIZE)\n","            sample = np.expand_dims(sample,-1) #the cnn needs 3 dimensions to work\n","            data += [(sample, label)]\n","            labels += [label]\n","\n","        progress +=1\n","        print('Progress: '+str(round(100*progress/len(dataset))) + '%', end=\"\\r\")\n","    return data, labels"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UeKWRzvgcI-u","colab_type":"code","colab":{}},"source":["def load_and_preprocess_lstm(dataset, SAMPLE_SIZE = 30):\n","    IMG_SIZE = (216,128) \n","    progress=0\n","\n","    data = []\n","    labels = []\n","    for (path, label) in dataset:\n","        audio, sr = librosa.load(path)\n","        dur = librosa.get_duration(audio, sr = sr)\n","        sampleNum = int(dur / SAMPLE_SIZE)\n","        offset = (dur % SAMPLE_SIZE) / 2\n","        for i in range(sampleNum):\n","            audio, sr = librosa.load(path, offset= offset+i, duration=SAMPLE_SIZE)\n","            sample = librosa.feature.melspectrogram(audio, sr=sr)\n","            # print(sample.shape)\n","            sample = cv2.resize(sample, dsize=IMG_SIZE)\n","            data += [(sample, label)]\n","            labels += [label]\n","        progress +=1\n","        print('\\r Progress: '+str(round(100*progress/len(dataset))) + '%', end='')\n","    return data, labels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IfJib9hgeopz","colab_type":"text"},"source":["---\n","\n","# Loading and preprocessing the samples for train test and validation data"]},{"cell_type":"code","metadata":{"id":"wnq2wmwYmRAj","colab_type":"code","colab":{}},"source":["def retrieve_samples(sample_size, model_type):\n","\n","    if model_type == 'cnn':\n","  \n","        print(\"\\nLoading train samples\")\n","        X_train_samples, train_labels = load_and_preprocess_cnn(X_train,sample_size)\n","        print(\"\\nLoading test samples\")\n","        X_test_samples, test_labels = load_and_preprocess_cnn(X_test,sample_size)\n","        print(\"\\nLoading val samples\")\n","        X_val_samples, val_labels = load_and_preprocess_cnn(X_val,sample_size)\n","    \n","    elif model_type == 'lstm':\n","\n","        print(\"\\nLoading train samples\")\n","        X_train_samples, train_labels = load_and_preprocess_lstm(X_train,sample_size)\n","        print(\"\\nLoading test samples\")\n","        X_test_samples, test_labels = load_and_preprocess_lstm(X_test,sample_size)\n","        print(\"\\nLoading val samples\")\n","        X_val_samples, val_labels = load_and_preprocess_lstm(X_val,sample_size)      \n","\n","\n","    # print(\"shape: \" + str(X_train_samples[0][0].shape))\n","    # print(\"number of training samples: \"+ str(len(X_train_samples)))\n","    # print(\"number of validation samples: \"+ str(len(X_val_samples)))\n","    # print(\"number of test samples: \"+ str(len(X_test_samples)))\n","\n","\n","    return X_train_samples, X_test_samples, X_val_samples"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"giHkx9wreoqC","colab_type":"text"},"source":["---\n","# Build the models"]},{"cell_type":"code","metadata":{"id":"Joggw8jjeoqH","colab_type":"code","colab":{}},"source":["#Define Model\n","\n","def create_cnn_model(input_shape):\n","  # input_shape = X_train_samples[0][0].shape\n","\n","  model = Sequential()\n","  model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n","  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","  model.add(GaussianNoise(0.1))\n","  model.add(Dropout(0.25))\n","  model.add(Flatten())\n","  model.add(Dense(128))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(NUM_CLASSES, activation='softmax')) #Compile\n","  model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.adam(), metrics=['accuracy'])\n","  # print(model.summary())\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TJZREA8xaGsp","colab_type":"code","colab":{}},"source":["def create_lstm_model(input_shape):\n","    model = Sequential()\n","\n","    model.add(LSTM(units = 512, dropout=0.5, recurrent_dropout=0.3, return_sequences = True, input_shape = (128, 216)))\n","    model.add(LSTM(units = 512, dropout=0.5, recurrent_dropout=0.3, return_sequences = False))\n","    model.add(Dense(units=NUM_CLASSES, activation='softmax'))#Compile\n","\n","    model.compile(loss=tensorflow.keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n","    # print(model.summary())\n","\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L4E2NVBJeoqK","colab_type":"text"},"source":["---\n","# Final data preperation for training models\n"]},{"cell_type":"code","metadata":{"id":"cBzZf3_lnsLX","colab_type":"code","colab":{}},"source":["def create_model_data_and_labels(X_train_samples, X_val_samples, X_test_samples):\n","    #Prepare samples to work for training the model\n","    labelizer = LabelEncoder()\n","\n","    #prepare training data and labels\n","    x_train = np.array([x[0] for x in X_train_samples])\n","    y_train = np.array([x[1] for x in X_train_samples])\n","    y_train = labelizer.fit_transform(y_train) \n","    y_train = to_categorical(y_train)\n","\n","    #prepare validation data and labels\n","    x_val = np.array([x[0] for x in X_val_samples])\n","    y_val = np.array([x[1] for x in X_val_samples])\n","    y_val = labelizer.transform(y_val)\n","    y_val = to_categorical(y_val)\n","\n","    #prepare test data and labels\n","    x_test = np.array([x[0] for x in X_test_samples])\n","    y_test = np.array([x[1] for x in X_test_samples])\n","    y_test = labelizer.transform(y_test)\n","    y_test = to_categorical(y_test)\n","\n","    return x_train, y_train, x_val, y_val, x_test, y_test, labelizer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"486bbUDReoqg","colab_type":"text"},"source":["---\n","# Metrics analysis & Visualization"]},{"cell_type":"code","metadata":{"id":"TY8VCv0Beoqq","colab_type":"code","colab":{}},"source":["def plot_data(history, ss):\n","    # list all data in history\n","    print(history.history.keys())\n","    # summarize history for accuracy\n","    sns.set(font_scale=1)\n","    plt.plot(history.history['accuracy'])\n","    plt.plot(history.history['val_accuracy'])\n","    plt.title('model accuracy')\n","    plt.ylabel('accuracy')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.show()\n","    plt.savefig('model_accuracy_size_'+str(ss)+'.png')\n","    # summarize history for loss\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('model loss')\n","    plt.ylabel('loss')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.show()\n","    plt.savefig('model_loss_size_'+str(ss)+'.png')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ax7n-jc_AE8o","colab_type":"code","colab":{}},"source":["plot_data(history)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SIF_B-7zeoqt","colab_type":"text"},"source":["## BEST model: do evaluation"]},{"cell_type":"code","metadata":{"id":"HhTGyc3910nT","colab_type":"code","colab":{}},"source":["def plot_conf_matrix(predictions, y_test, ss):\n","    matrix = confusion_matrix(labelizer.inverse_transform(y_test.argmax(axis=1)), \n","                          labelizer.inverse_transform(predictions.argmax(axis=1)), \n","                         labels=labelizer.classes_\n","                         )\n","\n","    matrix = pd.DataFrame(matrix, index=labelizer.classes_, columns=labelizer.classes_)\n","\n","    fig, ax = plt.subplots(figsize=(8, 8))\n","\n","    sns.set(font_scale=2.5)\n","    sns.heatmap(matrix, annot=True, fmt='.2f', linewidth=5, cmap=\"Greens\")\n","    ax.set_title('Confusion Matrix for (mis)classifications \\n with sample size: '+str(ss)+ ' seconds')\n","    ax.set_xlabel('Predictions')\n","    ax.set_ylabel('True values');\n","\n","    fig.savefig('conf_matrix_size_'+str(ss)+'.png', bbox_inches='tight') #"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gCO8El0WtLx0","colab_type":"code","colab":{}},"source":["\n","plot_conf_matrix(predictions, y_test,30)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"75FE8tZ1lTEB","colab_type":"text"},"source":["# Main Loop for the full procedure"]},{"cell_type":"code","metadata":{"id":"-GTFqO5keoq2","colab_type":"code","outputId":"b308cc17-2ddb-4541-ca75-e22dc1f3578c","executionInfo":{"status":"ok","timestamp":1588243726461,"user_tz":-120,"elapsed":422606,"user":{"displayName":"DL jopi","photoUrl":"","userId":"13040650484293272934"}},"colab":{"base_uri":"https://localhost:8080/","height":662}},"source":["#Main loop for testing multiple sample sizes\n","\n","#choose model type: cnn or ltsm\n","model_type = 'lstm'\n","\n","n_epochs = 200\n","patience = 10\n","es = EarlyStopping(patience=10)\n","sample_sizes = [5, 10, 15, 30]\n","start = timeit.default_timer()\n","\n","ModelData = pd.DataFrame(columns = ['Model Type','Sample size (s)', 'Time to Compute (s)',  'Early Stopping epoch', 'Training accuracy', 'Validation accuracy', 'Test Accuracy']) #create a DataFrame for storing the results \n","\n","conf_matrix_data = []\n","\n","for i in sample_sizes:\n","\n","    start_per_size = timeit.default_timer()\n","\n","    print(f'\\n---------- Model trained on samples of size: {i} seconds ----------------')\n","    X_train_samples, X_test_samples, X_val_samples = retrieve_samples(i,model_type)\n","    x_train, y_train, x_val, y_val, x_test, y_test, labelizer = create_model_data_and_labels(X_train_samples, X_val_samples, X_test_samples)\n","\n","    if model_type == 'cnn':\n","        model = create_cnn_model(X_train_samples[0][0].shape)\n","    elif model_type == 'lstm':\n","        model = create_lstm_model(X_train_samples[0][0].shape)\n","\n","\n","    history = model.fit(x_train, y_train, \n","              batch_size = 8, \n","              epochs=n_epochs,\n","              verbose=1, \n","              callbacks=[es],\n","              validation_data=(x_val, y_val))\n","    print('Finished training')\n","\n","\n","    early_stopping_epoch = len(history.history['accuracy'])\n","    training_accuracy = history.history['accuracy'][early_stopping_epoch-1-patience]\n","    validation_accuracy = history.history['val_accuracy'][early_stopping_epoch-1-patience]\n","\n","    plot_data(history, i)\n","\n","    predictions = model.predict(x_test)\n","    score = accuracy_score(labelizer.inverse_transform(y_test.argmax(axis=1)), labelizer.inverse_transform(predictions.argmax(axis=1)))\n","\n","    print('Sample size = ' + str(i) + ' seconds')\n","    print('Accuracy on test samples: ' + str(score))\n","    \n","    conf_matrix_data += [(predictions, y_test, i)]\n","\n","    stop_per_size = timeit.default_timer()\n","    time_to_compute = round(stop_per_size - start_per_size)\n","\n","    print ('Time to compute: '+str(time_to_compute))\n","\n","    ModelData.loc[len(ModelData)] = [model_type, i, time_to_compute, early_stopping_epoch, training_accuracy, validation_accuracy, score] #store particular settings configuration, early stoppping epoch and accuracies in dataframe\n","\n","stop = timeit.default_timer()\n","print ('\\ntime to compute: '+str(stop-start))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","---------- Model trained on samples of size: 5 seconds ----------------\n","\n","Loading train samples\n"," Progress: 100%\n","Loading test samples\n"," Progress: 100%\n","Loading val samples\n"," Progress: 100%WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","Train on 1723 samples, validate on 431 samples\n","Epoch 1/200\n","1723/1723 [==============================] - 655s 380ms/step - loss: 1.0057 - accuracy: 0.5740 - val_loss: 0.9675 - val_accuracy: 0.6032\n","Epoch 2/200\n","1723/1723 [==============================] - 634s 368ms/step - loss: 0.8851 - accuracy: 0.6274 - val_loss: 0.9186 - val_accuracy: 0.6148\n","Epoch 3/200\n","1723/1723 [==============================] - 633s 368ms/step - loss: 0.7788 - accuracy: 0.6831 - val_loss: 1.0645 - val_accuracy: 0.5847\n","Epoch 4/200\n","1723/1723 [==============================] - 614s 356ms/step - loss: 0.7344 - accuracy: 0.7046 - val_loss: 0.9916 - val_accuracy: 0.6148\n","Epoch 5/200\n","1723/1723 [==============================] - 607s 352ms/step - loss: 0.7130 - accuracy: 0.7115 - val_loss: 0.9633 - val_accuracy: 0.6311\n","Epoch 6/200\n","1723/1723 [==============================] - 612s 355ms/step - loss: 0.6806 - accuracy: 0.7191 - val_loss: 0.8977 - val_accuracy: 0.6497\n","Epoch 7/200\n","1723/1723 [==============================] - 641s 372ms/step - loss: 0.6478 - accuracy: 0.7435 - val_loss: 0.9853 - val_accuracy: 0.6288\n","Epoch 8/200\n","1723/1723 [==============================] - 619s 359ms/step - loss: 0.6146 - accuracy: 0.7516 - val_loss: 0.8865 - val_accuracy: 0.6357\n","Epoch 9/200\n","1723/1723 [==============================] - 612s 355ms/step - loss: 0.6044 - accuracy: 0.7644 - val_loss: 1.0519 - val_accuracy: 0.6821\n","Epoch 10/200\n","1723/1723 [==============================] - 585s 339ms/step - loss: 0.5897 - accuracy: 0.7667 - val_loss: 0.9694 - val_accuracy: 0.6775\n","Epoch 11/200\n"," 416/1723 [======>.......................] - ETA: 7:19 - loss: 0.5336 - accuracy: 0.7740"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ix2-iquXeoq4","colab_type":"code","colab":{}},"source":["print(ModelData)\n","\n","ModelData.head()\n","\n","import os.path\n","from google.colab import files\n","\n","\n","filenumber=1\n","while os.path.isfile('ModelData'+str(filenumber)+'.csv'): #find the first free file\n","    filenumber += 1\n","\n","ModelData.to_csv(r'ModelData'+str(filenumber)+'.csv') #store data in the first free csv file\n","files.download('ModelData'+str(filenumber)+'.csv') #download csv file "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lS6yC3pe6x7v","colab_type":"code","colab":{}},"source":["#reads a csv file and converts it to a LaTeX table\n","ModelDataLatex = pd.read_csv('ModelData1.csv')\n","print(ModelDataLatex.to_latex(index=False)) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZcKj_6--GAx5","colab_type":"code","colab":{}},"source":["#plot confusion matrices for the given input sample sizes\n","for j in conf_matrix_data:\n","    plot_conf_matrix(j[0], j[1], j[2])\n"],"execution_count":0,"outputs":[]}]}